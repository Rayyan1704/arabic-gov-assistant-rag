"""
Comprehensive 100-query test
50 formal queries + 50 real-world messy queries
"""

import sys
import os
sys.path.insert(0, 'src')

import json
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from sklearn.metrics.pairwise import cosine_similarity
from translator import TranslationService


def test_comprehensive():
    """Test with 100 diverse queries"""
    
    print("="*80)
    print("COMPREHENSIVE 100-QUERY TEST")
    print("="*80)
    
    # Load system
    embeddings = np.load('index/embeddings.npy').astype('float32')
    faiss.normalize_L2(embeddings)
    
    with open('index/corpus_chunks.json', 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    with open('index/corpus_meta.json', 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
    translator = TranslationService()
    
    # Load formal queries (50)
    with open('experiments/test_queries_dataset.json', 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    formal_queries = dataset['queries'][:50]
    
    # Real-world messy queries (50 more)
    real_world_queries = [
        # Transportation (10)
        {'query': 'driving license', 'category': 'transportation', 'type': 'short_en'},
        {'query': 'Ø±Ø®ØµØ© Ø³ÙˆØ§Ù‚Ø©', 'category': 'transportation', 'type': 'short_ar'},
        {'query': 'how get limo license', 'category': 'transportation', 'type': 'broken_en'},
        {'query': 'Ø§Ø¨ÙŠ Ø§Ø·Ù„Ø¹ Ø±Ø®ØµØ© Ø³ÙŠØ§Ù‚Ø©', 'category': 'transportation', 'type': 'dialect_ar'},
        {'query': 'bus purchase qatar', 'category': 'transportation', 'type': 'short_en'},
        {'query': 'Ø´Ø­Ù† Ø¬ÙˆÙŠ', 'category': 'transportation', 'type': 'short_ar'},
        {'query': 'fish transport', 'category': 'transportation', 'type': 'short_en'},
        {'query': 'ØªØµØ±ÙŠØ­ Ù†Ù‚Ù„', 'category': 'transportation', 'type': 'short_ar'},
        {'query': 'cargo service', 'category': 'transportation', 'type': 'short_en'},
        {'query': 'Ø·Ù„Ø¨ ØªØ¹Ù…ÙŠÙ… Ø³ÙŠØ§Ø±Ø©', 'category': 'transportation', 'type': 'formal_ar'},
        
        # Education (10)
        {'query': 'school registration', 'category': 'education', 'type': 'short_en'},
        {'query': 'ØªØ³Ø¬ÙŠÙ„ Ù…Ø¯Ø±Ø³Ø©', 'category': 'education', 'type': 'short_ar'},
        {'query': 'register kid school', 'category': 'education', 'type': 'broken_en'},
        {'query': 'ÙˆÙŠÙ† Ø§Ø³Ø¬Ù„ ÙˆÙ„Ø¯ÙŠ', 'category': 'education', 'type': 'dialect_ar'},
        {'query': 'university admission', 'category': 'education', 'type': 'short_en'},
        {'query': 'Ù‚Ø¨ÙˆÙ„ Ø¬Ø§Ù…Ø¹Ø©', 'category': 'education', 'type': 'short_ar'},
        {'query': 'transcript request', 'category': 'education', 'type': 'short_en'},
        {'query': 'ÙƒØ´Ù Ø¯Ø±Ø¬Ø§Øª', 'category': 'education', 'type': 'short_ar'},
        {'query': 'quran competition', 'category': 'education', 'type': 'short_en'},
        {'query': 'Ù…Ø³Ø§Ø¨Ù‚Ø© Ù‚Ø±Ø§Ù†', 'category': 'education', 'type': 'short_ar'},
        
        # Health (10)
        {'query': 'find doctor', 'category': 'health', 'type': 'short_en'},
        {'query': 'Ø¯ÙƒØªÙˆØ±', 'category': 'health', 'type': 'short_ar'},
        {'query': 'nurse license', 'category': 'health', 'type': 'short_en'},
        {'query': 'Ø±Ø®ØµØ© Ù…Ù…Ø±Ø¶', 'category': 'health', 'type': 'short_ar'},
        {'query': 'hamad hospital', 'category': 'health', 'type': 'short_en'},
        {'query': 'Ù…Ø³ØªØ´ÙÙ‰ Ø­Ù…Ø¯', 'category': 'health', 'type': 'short_ar'},
        {'query': 'medical report', 'category': 'health', 'type': 'short_en'},
        {'query': 'ØªÙ‚Ø±ÙŠØ± Ø·Ø¨ÙŠ', 'category': 'health', 'type': 'short_ar'},
        {'query': 'doctor consultation', 'category': 'health', 'type': 'short_en'},
        {'query': 'Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ©', 'category': 'health', 'type': 'short_ar'},
        
        # Business (10)
        {'query': 'business license', 'category': 'business', 'type': 'short_en'},
        {'query': 'Ø±Ø®ØµØ© ØªØ¬Ø§Ø±ÙŠØ©', 'category': 'business', 'type': 'short_ar'},
        {'query': 'company registration', 'category': 'business', 'type': 'short_en'},
        {'query': 'ØªØ³Ø¬ÙŠÙ„ Ø´Ø±ÙƒØ©', 'category': 'business', 'type': 'short_ar'},
        {'query': 'tenders', 'category': 'business', 'type': 'short_en'},
        {'query': 'Ù…Ù†Ø§Ù‚ØµØ§Øª', 'category': 'business', 'type': 'short_ar'},
        {'query': 'tax registration', 'category': 'business', 'type': 'short_en'},
        {'query': 'ØªØ³Ø¬ÙŠÙ„ Ø¶Ø±ÙŠØ¨ÙŠ', 'category': 'business', 'type': 'short_ar'},
        {'query': 'patent', 'category': 'business', 'type': 'short_en'},
        {'query': 'Ø¨Ø±Ø§Ø¡Ø© Ø§Ø®ØªØ±Ø§Ø¹', 'category': 'business', 'type': 'short_ar'},
        
        # Housing (5)
        {'query': 'rent allowance', 'category': 'housing', 'type': 'short_en'},
        {'query': 'Ø¨Ø¯Ù„ Ø§ÙŠØ¬Ø§Ø±', 'category': 'housing', 'type': 'short_ar'},
        {'query': 'property title', 'category': 'housing', 'type': 'short_en'},
        {'query': 'Ø³Ù†Ø¯ Ù…Ù„ÙƒÙŠØ©', 'category': 'housing', 'type': 'short_ar'},
        {'query': 'land lease', 'category': 'housing', 'type': 'short_en'},
        
        # Justice (5)
        {'query': 'court case', 'category': 'justice', 'type': 'short_en'},
        {'query': 'Ù‚Ø¶ÙŠØ© Ù…Ø­ÙƒÙ…Ø©', 'category': 'justice', 'type': 'short_ar'},
        {'query': 'lawyer portal', 'category': 'justice', 'type': 'short_en'},
        {'query': 'Ø¨ÙˆØ§Ø¨Ø© Ù…Ø­Ø§Ù…ÙŠÙ†', 'category': 'justice', 'type': 'short_ar'},
        {'query': 'legal clinic', 'category': 'justice', 'type': 'short_en'},
        
        # Culture (3)
        {'query': 'radio license', 'category': 'culture', 'type': 'short_en'},
        {'query': 'ØªØ±Ø®ÙŠØµ Ø±Ø§Ø¯ÙŠÙˆ', 'category': 'culture', 'type': 'short_ar'},
        {'query': 'film permit', 'category': 'culture', 'type': 'short_en'},
        
        # Info (2)
        {'query': 'hukoomi', 'category': 'info', 'type': 'short_en'},
        {'query': 'Ø­ÙƒÙˆÙ…ÙŠ', 'category': 'info', 'type': 'short_ar'},
    ]
    
    print(f"\nðŸ“Š Test Set:")
    print(f"   Formal queries: {len(formal_queries)}")
    print(f"   Real-world queries: {len(real_world_queries)}")
    print(f"   Total: {len(formal_queries) + len(real_world_queries)}")
    
    # Test both sets
    all_results = []
    
    print("\n" + "="*80)
    print("TESTING FORMAL QUERIES (50)")
    print("="*80)
    
    formal_correct = 0
    for i, test_query in enumerate(formal_queries, 1):
        query = test_query['query_ar']
        expected = test_query['category']
        
        query_emb = model.encode([query])[0].astype('float32').reshape(1, -1)
        faiss.normalize_L2(query_emb)
        
        sims = cosine_similarity(query_emb, embeddings)[0]
        top_idx = np.argmax(sims)
        predicted = metadata[top_idx]['category']
        
        is_correct = predicted == expected
        if is_correct:
            formal_correct += 1
        
        all_results.append({
            'query': query,
            'type': 'formal',
            'lang': 'AR',
            'expected': expected,
            'predicted': predicted,
            'correct': is_correct,
            'score': float(sims[top_idx])
        })
        
        if i % 10 == 0:
            print(f"   Progress: {i}/50 ({formal_correct}/{i} correct)")
    
    print(f"\n   Formal Accuracy: {formal_correct/len(formal_queries):.1%} ({formal_correct}/{len(formal_queries)})")
    
    print("\n" + "="*80)
    print("TESTING REAL-WORLD QUERIES (50)")
    print("="*80)
    
    real_correct = 0
    for i, test_query in enumerate(real_world_queries, 1):
        query = test_query['query']
        expected = test_query['category']
        query_type = test_query['type']
        
        # Detect language and translate if needed
        is_english = any(c.isascii() and c.isalpha() for c in query)
        
        if is_english:
            arabic_query = translator.translate_to_arabic(query)
            query_to_search = arabic_query
            lang = "EN"
        else:
            query_to_search = query
            lang = "AR"
        
        query_emb = model.encode([query_to_search])[0].astype('float32').reshape(1, -1)
        faiss.normalize_L2(query_emb)
        
        sims = cosine_similarity(query_emb, embeddings)[0]
        top_idx = np.argmax(sims)
        predicted = metadata[top_idx]['category']
        
        is_correct = predicted == expected
        if is_correct:
            real_correct += 1
        
        all_results.append({
            'query': query,
            'type': query_type,
            'lang': lang,
            'expected': expected,
            'predicted': predicted,
            'correct': is_correct,
            'score': float(sims[top_idx])
        })
        
        if i % 10 == 0:
            print(f"   Progress: {i}/50 ({real_correct}/{i} correct)")
    
    print(f"\n   Real-world Accuracy: {real_correct/len(real_world_queries):.1%} ({real_correct}/{len(real_world_queries)})")
    
    # Overall results
    total_correct = formal_correct + real_correct
    total_queries = len(formal_queries) + len(real_world_queries)
    overall_accuracy = total_correct / total_queries
    
    print("\n" + "="*80)
    print("OVERALL RESULTS")
    print("="*80)
    
    print(f"\nðŸ“Š Accuracy by Query Type:")
    print(f"   Formal queries:     {formal_correct}/{len(formal_queries)} ({formal_correct/len(formal_queries):.1%})")
    print(f"   Real-world queries: {real_correct}/{len(real_world_queries)} ({real_correct/len(real_world_queries):.1%})")
    print(f"   OVERALL:            {total_correct}/{total_queries} ({overall_accuracy:.1%})")
    
    # By language
    en_results = [r for r in all_results if r['lang'] == 'EN']
    ar_results = [r for r in all_results if r['lang'] == 'AR']
    
    en_correct = sum(1 for r in en_results if r['correct'])
    ar_correct = sum(1 for r in ar_results if r['correct'])
    
    print(f"\nðŸ“Š Accuracy by Language:")
    print(f"   English: {en_correct}/{len(en_results)} ({en_correct/len(en_results):.1%})")
    print(f"   Arabic:  {ar_correct}/{len(ar_results)} ({ar_correct/len(ar_results):.1%})")
    
    # By category
    from collections import defaultdict
    category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
    
    for result in all_results:
        cat = result['expected']
        category_stats[cat]['total'] += 1
        if result['correct']:
            category_stats[cat]['correct'] += 1
    
    print(f"\nðŸ“Š Accuracy by Category:")
    for cat in sorted(category_stats.keys()):
        stats = category_stats[cat]
        acc = stats['correct'] / stats['total']
        print(f"   {cat:15s}: {stats['correct']:2d}/{stats['total']:2d} ({acc:.1%})")
    
    # Failure analysis
    failures = [r for r in all_results if not r['correct']]
    
    print(f"\nâŒ Failures: {len(failures)}/{total_queries}")
    if failures:
        print("\nTop 10 failures:")
        for i, fail in enumerate(failures[:10], 1):
            print(f"   [{i}] '{fail['query'][:40]:40s}' ({fail['type']:12s})")
            print(f"       Expected: {fail['expected']:15s} Got: {fail['predicted']}")
    
    # Save comprehensive report
    report = {
        'summary': {
            'total_queries': total_queries,
            'formal_queries': len(formal_queries),
            'real_world_queries': len(real_world_queries),
            'total_correct': total_correct,
            'overall_accuracy': overall_accuracy,
            'formal_accuracy': formal_correct / len(formal_queries),
            'real_world_accuracy': real_correct / len(real_world_queries)
        },
        'by_language': {
            'english': {'correct': en_correct, 'total': len(en_results), 'accuracy': en_correct/len(en_results)},
            'arabic': {'correct': ar_correct, 'total': len(ar_results), 'accuracy': ar_correct/len(ar_results)}
        },
        'by_category': {cat: {'correct': stats['correct'], 'total': stats['total'], 
                              'accuracy': stats['correct']/stats['total']} 
                       for cat, stats in category_stats.items()},
        'all_results': all_results,
        'failures': failures
    }
    
    with open('index/comprehensive_100_test.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ’¾ Detailed report saved to: index/comprehensive_100_test.json")
    
    # Research quality assessment
    print("\n" + "="*80)
    print("RESEARCH QUALITY ASSESSMENT")
    print("="*80)
    
    if overall_accuracy >= 0.90:
        quality = "EXCELLENT"
        emoji = "ðŸ†"
    elif overall_accuracy >= 0.85:
        quality = "VERY GOOD"
        emoji = "âœ¨"
    elif overall_accuracy >= 0.80:
        quality = "GOOD"
        emoji = "âœ“"
    else:
        quality = "NEEDS IMPROVEMENT"
        emoji = "âš ï¸"
    
    print(f"\n{emoji} Overall Quality: {quality}")
    print(f"   100-query test set: âœ“ Sufficient for research")
    print(f"   Diverse query types: âœ“ Formal + Real-world")
    print(f"   Multiple languages: âœ“ English + Arabic")
    print(f"   Category coverage: âœ“ All 8 categories")
    
    if overall_accuracy >= 0.85:
        print(f"\nâœ“ This system is PUBLICATION-READY!")
        print(f"  - Strong accuracy across query types")
        print(f"  - Handles real-world messy queries")
        print(f"  - Bilingual support validated")
    
    return report


if __name__ == "__main__":
    report = test_comprehensive()
