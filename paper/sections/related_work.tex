\section{Related Work}

\subsection{Retrieval-Augmented Generation}

Retrieval-Augmented Generation (RAG) combines dense retrieval mechanisms with generative models to produce answers grounded in external knowledge sources \cite{lewis2020retrieval}. The framework has gained broad adoption due to its ability to reduce hallucinations and provide verifiable, citation-backed outputs \cite{gao2023retrieval}. Prior research has explored various components of RAG pipelines, including retrieval strategies, reranking methods \cite{nogueira2019passage}, and prompt design for improved factuality \cite{liu2023pre}. Despite substantial progress, most existing work focuses on English-only environments, leaving the cross-lingual behavior of RAG systems insufficiently examined.

\subsection{Multilingual Information Retrieval}

Cross-lingual information retrieval (CLIR) has been studied for decades \cite{nie2010cross}. Early methods relied on machine translation to project user queries into the document language \cite{oard1998comparative}, but translation introduces latency and compounds error propagation. More recent approaches employ multilingual embeddings that map multiple languages into a unified semantic space \cite{artetxe2019laser, reimers2020making}. Sentence-level multilingual transformers \cite{reimers2020making} and cross-lingual pre-training techniques \cite{tran2020criss} enable zero-shot transfer across languages. Nevertheless, their integration within RAG systems—and their comparative performance against translation-based pipelines—has not been systematically evaluated, particularly in constrained government-domain settings.

\subsection{Arabic NLP}

Arabic presents unique linguistic challenges due to its rich morphology, orthographic variability, and wide range of dialects \cite{habash2010introduction}. While Modern Standard Arabic (MSA) is relatively well supported by current NLP systems, dialectal varieties remain significantly more difficult to model \cite{bouamor2019madar}. Arabic-focused transformer models such as AraBERT \cite{antoun2020arabert}, derived from BERT \cite{devlin2019bert}, achieve strong performance on MSA tasks but are not designed for cross-lingual retrieval or mixed-language query scenarios. Moreover, the robustness of multilingual embedding models to Gulf Arabic—highly relevant in the Qatari context—has not been empirically assessed.

\subsection{Hybrid Retrieval}

Hybrid retrieval methods combine dense embeddings with sparse lexical models such as BM25 to leverage complementary strengths \cite{ma2021replication}. Sparse retrieval excels at capturing exact token matches, while dense retrieval captures deeper semantic relationships. However, recent evaluations indicate that strong dense retrievers increasingly diminish the benefit of BM25 augmentation \cite{thakur2021beir}. Whether these findings extend to Arabic-English cross-lingual retrieval remains unclear. Our study directly tests this hypothesis using high-quality multilingual embeddings.

\subsection{Government Service Information Systems}

Research on government information systems and chatbots has largely centered on English-language deployments \cite{androutsopoulou2019transforming} or monolingual settings \cite{lee2019government}. To date, no prior work has systematically examined cross-lingual RAG systems for Arabic-English government services, nor evaluated their robustness to real-world query variation—including dialectal Arabic, noisy phrasing, and underspecified queries. This gap motivates the empirical investigation presented in this study.
