\section{Results}

\textit{Note: Unless otherwise specified, accuracy metrics in this section refer to category-level accuracy (whether the retrieved document is from the correct service category). Section~\ref{sec:source_accuracy} presents exact source accuracy (whether the system retrieves the specific target document).}

\subsection{Translation Strategies (RQ1)}

To answer RQ1, we compare the retrieval performance of multilingual embeddings against translation-based approaches. Table~\ref{tab:translation} shows that multilingual embeddings match direct English embeddings at 100\% P@1 without any translation step. Translation-based methods perform worse (83.3\%) and add significant latency.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Method} & \textbf{P@1} & \textbf{P@3} & \textbf{MRR} & \textbf{Time (s)} \\
\hline
Direct English & 100\% & 100\% & 1.000 & 0.13 \\
Multilingual & 100\% & 100\% & 1.000 & 0.11 \\
Translate + Embed & 83.3\% & 83.3\% & 0.833 & 0.34 \\
Back-translation & 83.3\% & 91.7\% & 0.861 & 1.14 \\
\hline
\end{tabular}
\caption{Translation strategy comparison (12 English queries). Multilingual embeddings match direct English performance while eliminating translation overhead.}
\label{tab:translation}
\end{table}

\textbf{Answer to RQ1:} Yes. Multilingual embeddings achieve equivalent accuracy to direct English embeddings (100\%) and outperform translation-based methods (83.3\%) while reducing latency by 3x compared to translate-then-embed.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig1_translation_strategies.pdf}
\caption{Translation strategy evaluation. (a) Multilingual embeddings match direct English at 100\% accuracy. (b) Multilingual approach is 3x faster than translate-then-embed and 10x faster than back-translation.}
\label{fig:translation}
\end{figure}

\subsection{Hybrid Retrieval (RQ2)}

To answer RQ2, we compare pure semantic search against BM25-augmented hybrid configurations. Table~\ref{tab:hybrid} shows that hybrid 70/30 achieves the highest P@1 (92\%), slightly outperforming pure semantic (90\%).

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\hline
\textbf{Method} & \textbf{P@1} & \textbf{P@3} & \textbf{P@5} & \textbf{MRR} & \textbf{Time (s)} \\
\hline
Semantic Only & 90\% & 94\% & 94\% & 0.922 & 0.17 \\
BM25 Only & 52\% & 84\% & 88\% & 0.688 & 0.0003 \\
Hybrid 70/30 & 92\% & 98\% & 98\% & 0.949 & 0.15 \\
Hybrid 50/50 & 86\% & 98\% & 98\% & 0.912 & 0.15 \\
Cascade & 90\% & 94\% & 94\% & 0.922 & 0.15 \\
\hline
\end{tabular}
\caption{Hybrid retrieval comparison (50 Arabic queries). Hybrid 70/30 slightly outperforms pure semantic search.}
\label{tab:hybrid}
\end{table}

\textbf{Answer to RQ2:} Marginally yes. Hybrid retrieval (70/30 weighting) achieves 92\% P@1 compared to 90\% for pure semantic search. However, the improvement is small (+2\%) and domain-specific keyword boosting (+8\%) provides a larger gain. For simplicity, we recommend semantic search with keyword boosting over hybrid approaches.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig2_hybrid_retrieval.pdf}
\caption{Hybrid retrieval performance across five methods. Hybrid 70/30 achieves the highest P@1 (92\%), marginally outperforming pure semantic search (90\%). BM25-only performs poorly (52\% P@1).}
\label{fig:hybrid}
\end{figure}

\subsection{Robustness Analysis (RQ3)}

To answer RQ3, we evaluate the system on 100 messy queries across four categories. Table~\ref{tab:robustness} shows performance by query type.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Query Type} & \textbf{Count} & \textbf{Accuracy} & \textbf{Drop from Formal} \\
\hline
Formal (baseline) & 100 & 99\% & — \\
Dialectal Arabic & 30 & 90\% & -9\% \\
Short phrases & 25 & 84\% & -15\% \\
Broken grammar & 25 & 80\% & -19\% \\
Single words & 20 & 80\% & -19\% \\
\hline
\textbf{All messy} & 100 & 84\% & -15\% \\
\hline
\end{tabular}
\caption{Robustness results across query types. Dialectal Arabic shows the smallest degradation.}
\label{tab:robustness}
\end{table}

\textbf{Answer to RQ3:} The system degrades gracefully. Dialectal Arabic achieves 90\% (only 9\% drop), while single-word queries show 19\% degradation. Overall messy query category accuracy is 84\%, a 15\% drop from formal queries. Source accuracy reaches 78\% at P@5.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig3_robustness_analysis.pdf}
\caption{Robustness analysis. (a) Category accuracy by query type shows dialectal Arabic performs best among messy queries (90\%). (b) Source accuracy comparison reveals formal queries achieve 84\% P@1 while messy queries reach 78\% at P@5.}
\label{fig:robustness}
\end{figure}

\subsection{Ablation Study (RQ4)}

To answer RQ4, we measure the contribution of each system component. Table~\ref{tab:ablation} shows the impact of removing keyword boosting and title matching.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Configuration} & \textbf{Accuracy} & \textbf{Impact} \\
\hline
Full System & 99\% & baseline \\
Without Keyword Boosting & 91\% & -8\%$^\dagger$ \\
Without Title Matching & 99\% & 0\% \\
Pure Semantic Baseline & 91\% & — \\
\hline
\end{tabular}
\caption{Ablation study results (100 queries). $^\dagger$ indicates statistically significant difference (p < 0.05).}
\label{tab:ablation}
\end{table}

\textbf{Answer to RQ4:} Keyword boosting is the primary contributor (+8\% accuracy). Title matching has no measurable impact (0\%). Domain-specific keyword boosting is a simple, effective enhancement.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig4_system_comparison.pdf}
\caption{System performance and ablation study. (a) Our system achieves 99\% category accuracy and 84\% source accuracy, significantly outperforming BM25 baseline (56\%) with a 77\% relative gain. (b) Ablation study shows keyword boosting contributes +8\% while title matching has no impact.}
\label{fig:system}
\end{figure}

\subsection{Statistical Validation}

The full system (99\%) significantly outperforms the BM25 baseline (56\%), with p < 0.0001 (paired t-test) and large effect size (Cohen's d > 0.8). Table~\ref{tab:overall} summarises overall category-level performance (whether the top result is from the correct service category).

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\hline
\textbf{System} & \textbf{P@1} & \textbf{P@3} & \textbf{MRR} & \textbf{Time (s)} & \textbf{95\% CI} \\
\hline
Our System & 99\% & 99\% & 0.970 & 0.17 & [0.921, 0.999] \\
BM25 Baseline & 56\% & 70\% & 0.638 & 0.0003 & [0.512, 0.764] \\
\hline
\end{tabular}
\caption{Overall performance comparison (100 formal queries). Our system significantly outperforms BM25 (p < 0.0001). Response time includes retrieval only (not LLM generation).}
\label{tab:overall}
\end{table}

\subsection{Language and Category Breakdown}

Arabic and English queries perform comparably: 100\% (50/50) for Arabic, 98\% (49/50) for English. The difference is not statistically significant (p = 0.32). All categories achieve 100\% accuracy except business (93.75\%, 15/16), where the single failure involved a multi-domain query spanning business and education.

\subsection{Category vs Source Accuracy}
\label{sec:source_accuracy}

The metrics above report category-level accuracy (retrieved document from correct category). We also measured exact source accuracy—whether the system retrieves the specific correct document.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Query Type} & \textbf{Cat P@1} & \textbf{Cat P@3} & \textbf{Src P@1} & \textbf{Src P@3} \\
\hline
Formal Queries & 99\% & 99\% & 84\% & 92\% \\
Messy Queries & 84\% & 89\% & 51\% & 69\% \\
\hline
\end{tabular}
\caption{Category vs exact source accuracy. ``Cat'' = correct service category retrieved; ``Src'' = exact correct document retrieved. Category accuracy measures whether the top result is from the correct service category, while source accuracy measures whether the system retrieves the specific target document.}
\label{tab:source_accuracy}
\end{table}

On formal queries, the 15\% gap at P@1 reflects cases where the system retrieves a semantically related document from the correct category but not the exact target. Source accuracy improves to 92\% at P@3.

Notably, even on messy queries (single words, broken grammar, dialectal Arabic), the system achieves 51\% exact source accuracy at P@1, 69\% at P@3, and 78\% at P@5. This demonstrates that the multilingual embeddings capture sufficient semantic information to identify specific documents even from noisy, incomplete inputs.
