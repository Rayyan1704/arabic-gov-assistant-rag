% Workshop/Conference Paper Template
% 8 pages maximum for workshop, adjust as needed

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{multirow}

\title{Cross-Lingual Retrieval-Augmented Generation for Arabic Government Services}

\author{Your Name \\
  Your Institution \\
  \texttt{your.email@institution.edu} \\}

\begin{document}
\maketitle

\begin{abstract}
Retrieval-Augmented Generation (RAG) systems face unique challenges when applied to Arabic government services due to morphological complexity, limited resources, and cross-lingual query requirements. We present a RAG system that achieves 96\% accuracy on bilingual (Arabic-English) queries for Qatar government services. Through systematic experiments, we demonstrate that: (1) multilingual embeddings eliminate the need for explicit translation, achieving 100\% accuracy on English queries; (2) pure semantic search outperforms BM25 hybrid approaches for high-quality embeddings; (3) keyword boosting contributes +7\% accuracy for domain-specific retrieval. Our system shows a 71\% relative improvement (40 percentage points) over BM25 baseline with statistical significance (p < 0.0001). We release our dataset of 100 labeled queries and provide reproducible experimental methodology.
\end{abstract}

\section{Introduction}

% TODO: Expand this section
% - Problem: Arabic government service information retrieval
% - Challenges: Morphology, cross-lingual, limited resources
% - Solution: RAG with multilingual embeddings + keyword boosting
% - Contributions

Accessing government service information remains challenging for citizens, particularly in multilingual contexts. Arabic presents unique challenges including rich morphology, dialectal variation, and limited NLP resources. We address the problem of cross-lingual retrieval for Arabic government services.

\textbf{Contributions:}
\begin{itemize}
    \item Systematic comparison of translation strategies for Arabic-English RAG
    \item Evaluation of hybrid retrieval approaches (semantic + BM25)
    \item Ablation study quantifying component contributions
    \item 100-query bilingual benchmark with statistical validation
\end{itemize}

\section{Related Work}

% TODO: Fill in after reading papers
% Structure:
% 2.1 RAG Systems
% 2.2 Arabic NLP
% 2.3 Multilingual Retrieval
% 2.4 Hybrid Search

\subsection{Retrieval-Augmented Generation}

[Cite: Lewis et al. 2020, Guu et al. 2020, Karpukhin et al. 2020]

\subsection{Arabic NLP}

[Cite: Antoun et al. 2020 (AraBERT), Guellil et al. 2021]

\subsection{Multilingual Retrieval}

[Cite: Reimers & Gurevych 2020, Artetxe & Schwenk 2019]

\subsection{Hybrid Search and Reranking}

[Cite: Gao et al. 2021, Nogueira & Cho 2019]

\section{Methodology}

\subsection{System Architecture}

Our system consists of four main components:

\textbf{1. Document Processing:} 51 Arabic government service documents across 8 categories (transportation, education, health, business, housing, justice, culture, info).

\textbf{2. Embedding Generation:} We use paraphrase-multilingual-mpnet-base-v2 \cite{reimers2020multilingual} to generate 768-dimensional embeddings.

\textbf{3. Retrieval:} FAISS IndexFlatIP with cosine similarity. We implement keyword boosting for domain-specific terms with contextual boost factors (1.5× to 5.0×).

\textbf{4. Generation:} Google Gemini 2.0 Flash generates contextual answers from retrieved documents.

\subsection{Keyword Boosting}

We apply contextual keyword boosting to improve domain-specific retrieval:

\begin{equation}
score_{final} = 0.35 \cdot sim_{title} + 0.40 \cdot sim_{semantic} + 0.25 \cdot (sim_{semantic} \cdot boost)
\end{equation}

where $boost \in [1.5, 5.0]$ based on keyword importance.

\subsection{Dataset}

\textbf{Documents:} 51 Arabic government service documents  
\textbf{Test Queries:} 100 labeled queries (50 Arabic + 50 English)  
\textbf{Categories:} 8 (balanced distribution)  
\textbf{Difficulty:} Easy (40\%), Medium (45\%), Hard (15\%)

\section{Experiments}

\subsection{Experimental Setup}

\textbf{Metrics:} Precision@K, Mean Reciprocal Rank (MRR), NDCG@5, response time

\textbf{Baselines:} BM25 keyword search, pure semantic search

\textbf{Statistical Testing:} t-test, 95\% confidence intervals, multiple trials (n=3)

\subsection{Experiment 1: Translation Strategies}

We compare four translation approaches for English queries:

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{P@1} & \textbf{Time (s)} \\
\midrule
Direct English & 100\% & 0.13 \\
Multilingual & \textbf{100\%} & \textbf{0.11} \\
Translate + Embed & 83.3\% & 0.34 \\
Back-translation & 83.3\% & 1.14 \\
\bottomrule
\end{tabular}
\caption{Translation strategy comparison (12 English queries)}
\label{tab:translation}
\end{table}

\textbf{Finding:} Multilingual embeddings achieve 100\% accuracy without translation overhead.

\subsection{Experiment 2: Hybrid Retrieval}

We test five hybrid configurations:

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{P@1} & \textbf{Time (s)} \\
\midrule
Semantic Only & \textbf{84\%} & 0.14 \\
BM25 Only & 56\% & 0.0003 \\
Hybrid 70/30 & 80\% & 0.10 \\
Hybrid 50/50 & 70\% & 0.11 \\
Cascade & 84\% & 0.11 \\
\bottomrule
\end{tabular}
\caption{Hybrid retrieval comparison (50 Arabic queries)}
\label{tab:hybrid}
\end{table}

\textbf{Finding:} Pure semantic search outperforms hybrid approaches for high-quality embeddings.

\subsection{Experiment 3: Comprehensive Evaluation}

We evaluate on 100 queries (50 Arabic + 50 English):

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{95\% CI} \\
\midrule
Precision@1 & 96.0\% & [92.1, 99.9] \\
Precision@3 & 98.0\% & - \\
Precision@5 & 98.0\% & - \\
MRR & 0.970 & - \\
NDCG@5 & 2.313 & - \\
Response Time & 0.16s & - \\
\bottomrule
\end{tabular}
\caption{System performance on 100-query test set}
\label{tab:comprehensive}
\end{table}

\textbf{Baseline Comparison:}
\begin{itemize}
    \item Our System: 96.0\%
    \item BM25 Baseline: 56.0\%
    \item Improvement: +40pp (71\% relative)
    \item Statistical significance: t=8.52, p<0.0001
\end{itemize}

\textbf{Cross-Lingual Performance:}
\begin{itemize}
    \item Arabic: 96.0\% (48/50)
    \item English: 96.0\% (48/50)
    \item No language bias detected
\end{itemize}

\subsection{Experiment 4: Ablation Study}

We measure individual component contributions:

\begin{table}[h]
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{Accuracy} \\
\midrule
Full System & \textbf{96.0\%} \\
Without Keyword Boosting & 89.0\% \\
Without Title Matching & 96.0\% \\
Pure Semantic (Baseline) & 89.0\% \\
\midrule
\textit{Contributions:} & \\
Keyword Boosting & +7.0\% \\
Title Matching & 0.0\% \\
\bottomrule
\end{tabular}
\caption{Ablation study results (100 queries)}
\label{tab:ablation}
\end{table}

\textbf{Translation Impact (English queries):}
\begin{itemize}
    \item With translation: 100\% (10/10)
    \item Without translation: 90\% (9/10)
    \item Contribution: +10\%
\end{itemize}

\section{Results and Analysis}

\subsection{Overall Performance}

Our system achieves 96\% accuracy with statistical significance (p<0.0001) compared to BM25 baseline (56\%). The 95\% confidence interval [92.1\%, 99.9\%] indicates robust performance.

\subsection{Category Analysis}

Performance by category:
\begin{itemize}
    \item 7/8 categories: 100\% accuracy
    \item Justice category: 50\% accuracy (4/8 queries)
\end{itemize}

\textbf{Failure Analysis:} All 4 failures occurred in the justice category due to:
\begin{itemize}
    \item Limited training data (4 documents vs 8+ for other categories)
    \item Complex legal terminology
    \item Category confusion with business/info domains
\end{itemize}

\subsection{Key Findings}

\textbf{1. Translation Not Required:} Multilingual embeddings (paraphrase-multilingual-mpnet-base-v2) achieve 100\% accuracy on English queries without translation, saving 0.8s latency.

\textbf{2. Semantic > Hybrid:} For high-quality embeddings, pure semantic search (84\%) outperforms BM25 hybrid (70-80\%). This contradicts conventional wisdom that hybrid is always better.

\textbf{3. Keyword Boosting Critical:} Domain-specific keyword boosting contributes +7\% accuracy, demonstrating the value of domain adaptation.

\textbf{4. Cross-Lingual Parity:} Equal performance on Arabic (96\%) and English (96\%) queries validates multilingual approach.

\section{Discussion}

\subsection{When Hybrid Search Helps}

Our results show hybrid search doesn't improve high-quality embeddings. We hypothesize hybrid search benefits:
\begin{itemize}
    \item Lower-quality embeddings
    \item Exact keyword matching requirements
    \item Larger corpora (1000+ documents)
\end{itemize}

\subsection{Domain-Specific Adaptation}

Keyword boosting (+7\%) demonstrates the value of domain knowledge. Future work could explore:
\begin{itemize}
    \item Fine-tuning embeddings on government services
    \item Learning boost factors automatically
    \item Category-specific embeddings
\end{itemize}

\subsection{Limitations}

\begin{itemize}
    \item Small corpus (51 documents)
    \item Single domain (Qatar government services)
    \item Justice category requires specialized handling
    \item No human evaluation conducted
\end{itemize}

\section{Conclusion}

We present a cross-lingual RAG system for Arabic government services achieving 96\% accuracy with statistical validation. Key contributions include: (1) demonstrating multilingual embeddings eliminate translation need; (2) showing pure semantic search outperforms hybrid for high-quality embeddings; (3) quantifying keyword boosting contribution (+7\%). Our system provides a 71\% relative improvement over BM25 baseline.

\textbf{Future Work:}
\begin{itemize}
    \item Expand justice category documents
    \item Human evaluation study
    \item Cross-domain evaluation
    \item Fine-tuned domain-specific embeddings
\end{itemize}

\section*{Acknowledgments}

% TODO: Add acknowledgments if needed

\bibliography{references}
\bibliographystyle{acl_natbib}

\end{document}
