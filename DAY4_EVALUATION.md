# Day 4: Evaluation & Experiments

## Overview

This is the **most critical day** - it separates you from tutorial-followers. You're doing real ML engineering: hypothesis testing, metric evaluation, and scientific experimentation.

## What You're Testing

### Hypothesis
"Different chunk sizes affect retrieval quality"

### Variables
- **Independent**: Chunk size (256, 512, 768, 1024)
- **Dependent**: Retrieval accuracy (P@1, P@3, P@5, MRR)
- **Control**: Same documents, same model, same test queries

## Evaluation Metrics

### Precision@1 (P@1)
**Question**: Is the top result from the correct category?
- **1.0** = Perfect (top result always correct)
- **0.5** = 50% of queries get correct top result
- **0.0** = Never gets it right

### Precision@3 (P@3)
**Question**: How many of the top 3 results are relevant?
- **1.0** = All top 3 are relevant
- **0.67** = 2 out of 3 are relevant
- **0.33** = 1 out of 3 is relevant

### Precision@5 (P@5)
**Question**: How many of the top 5 results are relevant?
- Similar to P@3 but for top 5

### Mean Reciprocal Rank (MRR)
**Question**: On average, what position is the first relevant result?
- **1.0** = Always rank 1
- **0.5** = Average rank 2
- **0.33** = Average rank 3

**Formula**: MRR = 1 / (position of first relevant result)

## Test Set (20 Queries)

### Transportation (5 queries)
```
ÙƒÙŠÙ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø±Ø®ØµØ© Ù„ÙŠÙ…ÙˆØ²ÙŠÙ†ØŸ
Ù…Ø§ Ù‡ÙŠ Ø®Ø·ÙˆØ§Øª ØªØ£Ø¬ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ø±Ø§ØªØŸ
ÙƒÙŠÙ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªØ±Ø®ÙŠØµ Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù…Ø§ÙƒØŸ
Ù…Ø§ Ù‡ÙŠ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø®ØµØ© Ø´Ø­Ù† Ø¬ÙˆÙŠØŸ
ÙƒÙŠÙ Ø£Ø·Ù„Ø¨ ØªØ¹Ù…ÙŠÙ… Ø¹Ù„Ù‰ Ù…Ø±ÙƒØ¨Ø©ØŸ
```

### Education (5 queries)
```
ÙƒÙŠÙ Ø£Ø³Ø¬Ù„ ÙÙŠ Ù…Ù‚Ø±Ø±Ø§Øª Ø¬Ø§Ù…Ø¹Ø© Ù‚Ø·Ø±ØŸ
Ù…Ø§ Ù‡ÙŠ Ø®Ø·ÙˆØ§Øª Ø·Ù„Ø¨ ÙƒØ´Ù Ø§Ù„Ø¯Ø±Ø¬Ø§ØªØŸ
ÙƒÙŠÙ Ø£ØªÙ‚Ø¯Ù… Ù„Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø­Ù…Ø¯ Ø¨Ù† Ø®Ù„ÙŠÙØ©ØŸ
ÙƒÙŠÙ Ø£Ù†Ø³Ø­Ø¨ Ù…Ù† Ø¬Ø§Ù…Ø¹Ø© Ù‚Ø·Ø±ØŸ
Ø£ÙŠÙ† Ø£Ø¬Ø¯ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø¨Ø­Ø«ÙŠØ© ÙÙŠ Ù‚Ø·Ø±ØŸ
```

### Health (5 queries)
```
ÙƒÙŠÙ Ø£Ø·Ù„Ø¨ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ©ØŸ
ÙƒÙŠÙ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø·Ø¨ÙŠ Ù…Ù† Ø­Ù…Ø¯ØŸ
ÙƒÙŠÙ Ø£ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù…Ø¤Ø³Ø³Ø© Ø­Ù…Ø¯ Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ø¬Ù„Ø©ØŸ
ÙƒÙŠÙ Ø£ØªÙ‚Ø¯Ù… Ù„Ù„ØªÙˆØ¸ÙŠÙ ÙÙŠ Ù…Ø¤Ø³Ø³Ø© Ø­Ù…Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ©ØŸ
ÙƒÙŠÙ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªØ±Ø®ÙŠØµ Ù…Ù…Ø§Ø±Ø³ ØµØ­ÙŠØŸ
```

### Business (5 queries)
```
ÙƒÙŠÙ Ø£Ù‚Ø¯Ù… Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ù†Ø§Ù‚ØµØ§ØªØŸ
ÙƒÙŠÙ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø´Ù‡Ø§Ø¯Ø© ØªØ£ÙƒÙŠØ¯ Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø·Ù„Ø¨ØŸ
ÙƒÙŠÙ Ø£Ø³Ø¬Ù„ Ù†ÙØ³ÙŠ ÙƒÙ…ÙƒÙ„Ù ÙÙŠ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ØŸ
ÙƒÙŠÙ Ø£Ø¹ÙŠØ¯ ØªÙØ¹ÙŠÙ„ Ø±Ø®ØµØ© ØªØ¬Ø§Ø±ÙŠØ©ØŸ
ÙƒÙŠÙ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªÙ…ÙˆÙŠÙ„ Ù…Ù† Ø¨Ù†Ùƒ Ù‚Ø·Ø± Ù„Ù„ØªÙ†Ù…ÙŠØ©ØŸ
```

## Experiment Configurations

| Config | Chunk Size | Overlap | Expected Chunks |
|--------|-----------|---------|-----------------|
| 1 | 256 | 64 | ~68 |
| 2 | 512 | 128 | ~34 (current) |
| 3 | 768 | 192 | ~23 |
| 4 | 1024 | 256 | ~17 |

## Expected Results

### Hypothesis Predictions

**Small chunks (256)**
- âœ… More granular matching
- âŒ May lose context
- âŒ More chunks = slower

**Medium chunks (512)** - Current
- âœ… Good balance
- âœ… Preserves context
- âœ… Fast retrieval

**Large chunks (768-1024)**
- âœ… Maximum context
- âŒ Less precise matching
- âœ… Fewer chunks

## How to Run

```bash
jupyter notebook notebooks/07_evaluation_experiments.ipynb
```

**Time**: ~15-20 minutes (depending on your machine)

## What You'll Learn

### 1. Evaluation Methodology
- How to create test sets
- How to measure retrieval quality
- How to compare configurations

### 2. Trade-offs
- Chunk size vs. accuracy
- Context vs. precision
- Speed vs. quality

### 3. Scientific Thinking
- Hypothesis â†’ Experiment â†’ Analysis
- Metric selection
- Error analysis

## Interpreting Results

### Good Results
- **P@1 > 0.8**: Excellent top-1 accuracy
- **P@3 > 0.7**: Good top-3 coverage
- **MRR > 0.8**: Relevant results appear early

### Acceptable Results
- **P@1 > 0.6**: Decent accuracy
- **P@3 > 0.5**: Reasonable coverage
- **MRR > 0.6**: Relevant results in top 2-3

### Poor Results
- **P@1 < 0.5**: Needs improvement
- **P@3 < 0.4**: Poor coverage
- **MRR < 0.5**: Relevant results too low

## Error Analysis

When a query fails (P@1 = 0), ask:

### 1. Is the query ambiguous?
```
"ÙƒÙŠÙ Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø±Ø®ØµØ©ØŸ"  # Which license?
```

### 2. Is the expected category wrong?
```
Query about "Ø´Ù‡Ø§Ø¯Ø© Ù…Ù† ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª"
Could be: transportation OR business
```

### 3. Is preprocessing too aggressive?
```
Original: "Ù„ÙŠÙ…ÙˆØ²ÙŠÙ†"
After normalization: Lost distinctive features?
```

### 4. Is the document missing keywords?
```
Query: "ØªØ£Ø¬ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª"
Document: Only mentions "Ù„ÙŠÙ…ÙˆØ²ÙŠÙ†" not "ØªØ£Ø¬ÙŠØ±"
```

## What Makes This Critical

### âŒ Tutorial Approach
- "Here's the code, run it"
- No testing
- No metrics
- No analysis

### âœ… Your Approach
- Created test set (20 queries)
- Tested 4 configurations
- Measured 4 metrics
- Analyzed failures
- **Shows you understand ML engineering**

## Deliverables

After running the notebook, you'll have:

1. **experiment_results.json** - All experiment data
2. **Results table** - Comparing configurations
3. **Best configuration** - Data-driven choice
4. **Error analysis** - Understanding failures
5. **Insights** - What works and why

## Next Steps

### If Results Are Good (P@1 > 0.8)
- âœ… Keep current configuration
- âœ… Document findings
- âœ… Move to deployment

### If Results Are Poor (P@1 < 0.6)
- ðŸ”§ Adjust preprocessing
- ðŸ”§ Try different embedding model
- ðŸ”§ Add query expansion
- ðŸ”§ Implement reranking

## Interview Questions You Can Answer

### "How did you evaluate your RAG system?"
"I created a test set of 20 queries with expected categories, then measured Precision@1, Precision@3, and MRR across different chunk sizes."

### "What chunk size did you choose and why?"
"I tested 4 configurations (256, 512, 768, 1024) and found that 512 with 128 overlap gave the best P@1 score of X.XX while maintaining good context."

### "How do you know your system works?"
"I have quantitative metrics: P@1 of X.XX means the top result is correct XX% of the time. I also did error analysis on failed queries."

### "What would you improve?"
"Based on error analysis, I found that [specific issue]. I would address this by [specific solution]."

## This Is What Separates You

- âœ… You tested hypotheses scientifically
- âœ… You measured with proper metrics
- âœ… You analyzed failures
- âœ… You made data-driven decisions
- âœ… You can explain your choices

**This is real ML engineering, not just following tutorials.**

## Resources

### Learn More About Metrics
- [Information Retrieval Metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))
- [Understanding MRR](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)
- [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)

### Similar Work
- [BEIR Benchmark](https://github.com/beir-cellar/beir)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [RAG Evaluation](https://www.pinecone.io/learn/rag-evaluation/)

---

**Ready to run the experiments?**

```bash
jupyter notebook notebooks/07_evaluation_experiments.ipynb
```

**Time to show you're not just following tutorials!** ðŸš€
