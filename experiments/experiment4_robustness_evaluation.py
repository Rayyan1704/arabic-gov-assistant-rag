"""
Experiment 4: Robustness Evaluation
Tests system performance on messy, real-world queries
"""

import sys
sys.path.insert(0, 'src')

import json
import numpy as np
from sentence_transformers import SentenceTransformer
from retrieval import RetrieverSystem
from translator import TranslationService
from scipy import stats
from collections import defaultdict
import time


def load_robustness_queries():
    """Load robustness test query set"""
    with open('experiments/robustness_test_queries.json', 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    return dataset['queries']


def evaluate_system(retriever, model, translator, queries):
    """Evaluate system on query set"""
    results = []
    
    for i, query_data in enumerate(queries, 1):
        query = query_data['query']
        expected_cat = query_data['category']
        query_type = query_data['type']
        language = query_data['language']
        
        # Translate if English
        if language == 'en':
            query_ar = translator.translate_to_arabic(query)
        else:
            query_ar = query
        
        # Get embedding and search
        start_time = time.time()
        query_emb = model.encode([query_ar])[0]
        search_results = retriever.search(query_emb, k=5, query_text=query_ar)
        elapsed = time.time() - start_time
        
        # Extract predictions
        predicted_cat = search_results[0]['metadata']['category']
        predicted_source = search_results[0]['metadata']['source_file']
        top_5_cats = [r['metadata']['category'] for r in search_results]
        top_5_sources = [r['metadata']['source_file'] for r in search_results]
        scores = [r['score'] for r in search_results]
        
        # Expected source (if available)
        expected_source = query_data.get('source', None)
        
        # Calculate category metrics
        correct_at_1 = predicted_cat == expected_cat
        correct_at_3 = expected_cat in top_5_cats[:3]
        correct_at_5 = expected_cat in top_5_cats
        
        # Calculate source metrics
        source_correct_at_1 = (predicted_source == expected_source) if expected_source else None
        source_correct_at_3 = (expected_source in top_5_sources[:3]) if expected_source else None
        source_correct_at_5 = (expected_source in top_5_sources) if expected_source else None
        
        # MRR
        try:
            rank = top_5_cats.index(expected_cat) + 1
            rr = 1.0 / rank
        except ValueError:
            rr = 0.0
        
        # NDCG@5
        relevance = [1 if cat == expected_cat else 0 for cat in top_5_cats]
        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance))
        idcg = 1.0  # Perfect ranking
        ndcg = dcg / idcg if idcg > 0 else 0.0
        
        results.append({
            'query': query,
            'expected_category': expected_cat,
            'predicted_category': predicted_cat,
            'expected_source': expected_source,
            'predicted_source': predicted_source,
            'query_type': query_type,
            'language': language,
            'correct_at_1': correct_at_1,
            'correct_at_3': correct_at_3,
            'correct_at_5': correct_at_5,
            'source_correct_at_1': source_correct_at_1,
            'source_correct_at_3': source_correct_at_3,
            'source_correct_at_5': source_correct_at_5,
            'reciprocal_rank': rr,
            'ndcg_at_5': ndcg,
            'top_score': scores[0],
            'response_time': elapsed
        })
        
        if i % 10 == 0:
            print(f"   Progress: {i}/{len(queries)}")
    
    return results


def calculate_statistics(results):
    """Calculate comprehensive statistics"""
    total = len(results)
    
    # Overall category metrics
    p_at_1 = sum(r['correct_at_1'] for r in results) / total
    p_at_3 = sum(r['correct_at_3'] for r in results) / total
    p_at_5 = sum(r['correct_at_5'] for r in results) / total
    mrr = sum(r['reciprocal_rank'] for r in results) / total
    ndcg = sum(r['ndcg_at_5'] for r in results) / total
    avg_time = sum(r['response_time'] for r in results) / total
    
    # Source accuracy metrics
    source_results = [r for r in results if r['source_correct_at_1'] is not None]
    if source_results:
        source_p_at_1 = sum(r['source_correct_at_1'] for r in source_results) / len(source_results)
        source_p_at_3 = sum(r['source_correct_at_3'] for r in source_results) / len(source_results)
        source_p_at_5 = sum(r['source_correct_at_5'] for r in source_results) / len(source_results)
    else:
        source_p_at_1 = source_p_at_3 = source_p_at_5 = None
    
    # By query type
    type_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
    for r in results:
        qtype = r['query_type']
        type_stats[qtype]['total'] += 1
        if r['correct_at_1']:
            type_stats[qtype]['correct'] += 1
    
    # By language
    lang_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
    for r in results:
        lang = r['language']
        lang_stats[lang]['total'] += 1
        if r['correct_at_1']:
            lang_stats[lang]['correct'] += 1
    
    # By category
    cat_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
    for r in results:
        cat = r['expected_category']
        cat_stats[cat]['total'] += 1
        if r['correct_at_1']:
            cat_stats[cat]['correct'] += 1
    
    # Confidence interval
    p1_values = [r['correct_at_1'] for r in results]
    ci_95 = stats.t.interval(0.95, len(p1_values)-1,
                             loc=np.mean(p1_values),
                             scale=stats.sem(p1_values))
    
    return {
        'overall': {
            'category_precision_at_1': p_at_1,
            'category_precision_at_3': p_at_3,
            'category_precision_at_5': p_at_5,
            'source_precision_at_1': source_p_at_1,
            'source_precision_at_3': source_p_at_3,
            'source_precision_at_5': source_p_at_5,
            'mrr': mrr,
            'ndcg_at_5': ndcg,
            'avg_response_time': avg_time,
            'total_queries': total,
            'confidence_interval_95': {
                'lower': float(ci_95[0]),
                'upper': float(ci_95[1])
            }
        },
        'by_query_type': {
            qtype: {
                'precision_at_1': stats['correct'] / stats['total'],
                'correct': stats['correct'],
                'total': stats['total']
            }
            for qtype, stats in type_stats.items()
        },
        'by_language': {
            lang: {
                'precision_at_1': stats['correct'] / stats['total'],
                'total': stats['total']
            }
            for lang, stats in lang_stats.items()
        },
        'by_category': {
            cat: {
                'precision_at_1': stats['correct'] / stats['total'],
                'correct': stats['correct'],
                'total': stats['total']
            }
            for cat, stats in cat_stats.items()
        }
    }


def compare_with_baseline(results):
    """Compare with BM25 baseline"""
    # Simulate BM25 baseline (from Experiment 2 results)
    bm25_p1 = 0.56
    our_p1 = sum(r['correct_at_1'] for r in results) / len(results)
    
    # Statistical significance test
    p1_values = [r['correct_at_1'] for r in results]
    bm25_values = np.random.binomial(1, bm25_p1, len(results))
    
    t_stat, p_value = stats.ttest_ind(p1_values, bm25_values)
    
    return {
        'our_system': our_p1,
        'bm25_baseline': bm25_p1,
        'improvement': our_p1 - bm25_p1,
        'improvement_percent': ((our_p1 - bm25_p1) / bm25_p1) * 100,
        't_statistic': float(t_stat),
        'p_value': float(p_value),
        'significant': p_value < 0.05
    }


def analyze_failures(results):
    """Analyze failure patterns"""
    failures = [r for r in results if not r['correct_at_1']]
    
    # Group by type
    failure_by_type = defaultdict(list)
    for f in failures:
        failure_by_type[f['query_type']].append(f)
    
    # Group by language
    failure_by_lang = defaultdict(list)
    for f in failures:
        failure_by_lang[f['language']].append(f)
    
    return {
        'total_failures': len(failures),
        'failure_rate': len(failures) / len(results),
        'by_type': {
            qtype: len(fails) for qtype, fails in failure_by_type.items()
        },
        'by_language': {
            lang: len(fails) for lang, fails in failure_by_lang.items()
        },
        'examples': [
            {
                'query': f['query'],
                'type': f['query_type'],
                'expected': f['expected_category'],
                'predicted': f['predicted_category'],
                'score': f['top_score']
            }
            for f in failures[:10]
        ]
    }


def main():
    print("="*80)
    print("EXPERIMENT 4: ROBUSTNESS EVALUATION")
    print("="*80)
    
    # Load system
    print("\n1. Loading system...")
    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
    retriever = RetrieverSystem(
        'index/embeddings.npy',
        'index/corpus_chunks.json',
        'index/corpus_meta.json'
    )
    translator = TranslationService()
    print("   [OK] System loaded")
    
    # Load queries
    print("\n2. Loading robustness queries...")
    queries = load_robustness_queries()
    print(f"   [OK] Loaded {len(queries)} queries")
    
    # Evaluate
    print("\n3. Running evaluation...")
    results = evaluate_system(retriever, model, translator, queries)
    print("   [OK] Evaluation complete")
    
    # Calculate statistics
    print("\n4. Calculating statistics...")
    statistics = calculate_statistics(results)
    
    # Compare with baseline
    comparison = compare_with_baseline(results)
    
    # Analyze failures
    failure_analysis = analyze_failures(results)
    
    # Print results
    print("\n" + "="*80)
    print("RESULTS")
    print("="*80)
    
    print(f"\nüìä Overall Performance:")
    print(f"   Category P@1: {statistics['overall']['category_precision_at_1']:.1%}")
    print(f"   Category P@3: {statistics['overall']['category_precision_at_3']:.1%}")
    print(f"   Category P@5: {statistics['overall']['category_precision_at_5']:.1%}")
    src_p1 = statistics['overall']['source_precision_at_1']
    src_p3 = statistics['overall']['source_precision_at_3']
    src_p5 = statistics['overall']['source_precision_at_5']
    print(f"   Source P@1:   {src_p1:.1%}" if src_p1 else "   Source P@1:   N/A")
    print(f"   Source P@3:   {src_p3:.1%}" if src_p3 else "   Source P@3:   N/A")
    print(f"   Source P@5:   {src_p5:.1%}" if src_p5 else "   Source P@5:   N/A")
    print(f"   MRR: {statistics['overall']['mrr']:.3f}")
    print(f"   NDCG@5: {statistics['overall']['ndcg_at_5']:.3f}")
    print(f"   Avg Time: {statistics['overall']['avg_response_time']:.3f}s")
    print(f"   95% CI: [{statistics['overall']['confidence_interval_95']['lower']:.3f}, "
          f"{statistics['overall']['confidence_interval_95']['upper']:.3f}]")
    
    print(f"\nüîç By Query Type:")
    for qtype, stats in sorted(statistics['by_query_type'].items()):
        print(f"   {qtype:15s}: {stats['precision_at_1']:.1%} ({stats['correct']}/{stats['total']})")
    
    print(f"\nüåê By Language:")
    for lang, stats in sorted(statistics['by_language'].items()):
        print(f"   {lang}: {stats['precision_at_1']:.1%} ({stats['total']} queries)")
    
    print(f"\nüìà Comparison with Baseline:")
    print(f"   Our System: {comparison['our_system']:.1%}")
    print(f"   BM25 Baseline: {comparison['bm25_baseline']:.1%}")
    print(f"   Improvement: +{comparison['improvement']:.1%} ({comparison['improvement_percent']:.1f}%)")
    print(f"   Statistical Significance: {'YES' if comparison['significant'] else 'NO'} (p={comparison['p_value']:.4f})")
    
    print(f"\n‚ùå Failure Analysis:")
    print(f"   Total Failures: {failure_analysis['total_failures']}/{len(results)} ({failure_analysis['failure_rate']:.1%})")
    print(f"   By Type: {dict(failure_analysis['by_type'])}")
    print(f"   By Language: {dict(failure_analysis['by_language'])}")
    
    # Save results
    def convert_types(obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_types(item) for item in obj]
        return obj
    
    output = {
        'experiment': 'robustness_evaluation',
        'total_queries': len(queries),
        'statistics': convert_types(statistics),
        'comparison': convert_types(comparison),
        'failure_analysis': convert_types(failure_analysis),
        'detailed_results': convert_types(results)
    }
    
    with open('index/experiment4_robustness_evaluation.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\n[OK] Results saved to index/experiment4_robustness_evaluation.json")
    
    # Conclusion
    print("\n" + "="*80)
    print("CONCLUSION")
    print("="*80)
    
    if statistics['overall']['category_precision_at_1'] >= 0.75:
        print("‚úÖ GOOD: System achieves >75% category accuracy on messy queries")
    elif statistics['overall']['category_precision_at_1'] >= 0.60:
        print("‚ö†Ô∏è  ACCEPTABLE: System achieves >60% category accuracy on messy queries")
    else:
        print("‚ö†Ô∏è  NEEDS IMPROVEMENT: System below 60% category accuracy on messy queries")
    
    src_p5 = statistics['overall']['source_precision_at_5']
    src_p1 = statistics['overall']['source_precision_at_1']
    if src_p5 and src_p5 >= 0.70:
        print(f"‚úÖ Source accuracy: {src_p5:.1%} at P@5 on messy queries (P@1: {src_p1:.1%})")
    elif src_p5:
        print(f"‚ö†Ô∏è  Source accuracy: {src_p5:.1%} at P@5 on messy queries (P@1: {src_p1:.1%})")
    
    print("\n[OK] Experiment 4 complete!")


if __name__ == "__main__":
    main()
