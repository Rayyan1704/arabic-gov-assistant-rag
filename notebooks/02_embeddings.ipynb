{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Generate Embeddings\n",
    "Convert text chunks into vector embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding Embeddings\n",
    "\n",
    "Test how embeddings capture semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"كيف أحصل على رخصة ليموزين؟\",  # How do I get limousine license?\n",
    "    \"ما هي متطلبات تأجير السيارات؟\",  # What are car rental requirements?\n",
    "    \"أريد تقديم طلب قبول جامعي\"  # I want to apply for university (different topic)\n",
    "]\n",
    "\n",
    "embeddings = model.encode(texts)\n",
    "print(f\"Shape: {embeddings.shape}\")  # Should be (3, 768)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"\\nSimilarity matrix:\")\n",
    "print(sim_matrix)\n",
    "print(\"\\n✅ First two should be more similar (both about transportation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Preprocessed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunks\n",
    "with open('../index/corpus_chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "with open('../index/corpus_meta.json', 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings for All Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating embeddings...\")\n",
    "embeddings_list = []\n",
    "\n",
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
    "    embeddings_list.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Expected: ({len(chunks)}, 768)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../index/embeddings.npy', embeddings)\n",
    "print(\"✅ Saved embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quick Quality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"كيف أحصل على خدمة الليموزين؟\"\n",
    "query_embedding = model.encode([test_query])[0]\n",
    "\n",
    "# Find most similar chunks\n",
    "similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "top_5_idx = np.argsort(similarities)[-5:][::-1]\n",
    "\n",
    "print(f\"Test query: {test_query}\")\n",
    "print(\"\\nTop 5 most similar chunks:\")\n",
    "for idx in top_5_idx:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Score: {similarities[idx]:.3f}\")\n",
    "    print(f\"Category: {metadata[idx]['category']}\")\n",
    "    print(f\"File: {metadata[idx]['source_file'].split('/')[-1]}\")\n",
    "    print(f\"Text: {chunks[idx][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries matching actual data\n",
    "test_queries = [\n",
    "    \"كيف أحصل على رخصة ليموزين؟\",  # limousine license\n",
    "    \"ما هي متطلبات تسجيل المقررات الجامعية؟\",  # university course registration\n",
    "    \"كيف أطلب استشارة طبية؟\",  # medical consultation\n",
    "    \"ما هي إجراءات تقديم العروض للمناقصات؟\",  # tender submission\n",
    "    \"كيف أحصل على شهادة تأكيد استلام الطلب؟\",  # CRA certificate\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    query_emb = model.encode([query])[0]\n",
    "    sims = cosine_similarity([query_emb], embeddings)[0]\n",
    "    top_idx = np.argmax(sims)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top match ({sims[top_idx]:.3f}):\")\n",
    "    print(f\"  Category: {metadata[top_idx]['category']}\")\n",
    "    print(f\"  File: {metadata[top_idx]['source_file'].split('/')[-1]}\")\n",
    "    print(f\"  Text: {chunks[top_idx][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Checkpoint\n",
    "\n",
    "Embeddings generated and saved. Ready for FAISS indexing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
